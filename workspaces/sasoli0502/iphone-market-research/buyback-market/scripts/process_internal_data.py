#!/usr/bin/env python3
"""
å¼Šç¤¾è²·å–å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Kintoneã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç«¶åˆåˆ†æç”¨ã«æ­£è¦åŒ–ã™ã‚‹ã€‚

ä½¿ç”¨æ–¹æ³•:
    uv run python scripts/process_internal_data.py data/internal/kintone_buyback_data_20251117.csv
"""

import sys
from pathlib import Path
import pandas as pd
from datetime import datetime

def load_kintone_data(file_path: str) -> pd.DataFrame:
    """
    Kintoneã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸCSVã‚’èª­ã¿è¾¼ã‚€

    Args:
        file_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

    Returns:
        pandas.DataFrame
    """
    print(f"ğŸ“‚ èª­ã¿è¾¼ã¿ä¸­: {file_path}")

    # Shift-JISã§èª­ã¿è¾¼ã¿ï¼ˆKintoneã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
    try:
        df = pd.read_csv(file_path, encoding='shift-jis')
        print(f"âœ… {len(df):,}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    except UnicodeDecodeError:
        # UTF-8ã‚‚è©¦ã™
        df = pd.read_csv(file_path, encoding='utf-8-sig')
        print(f"âœ… {len(df):,}ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸï¼ˆUTF-8ï¼‰")

    return df

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°

    - å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
    - ç©ºãƒ‡ãƒ¼ã‚¿ã®é™¤å¤–
    - iPhoneãƒ‡ãƒ¼ã‚¿ã®ã¿æŠ½å‡º
    - ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®æ•°å€¤åŒ–
    """
    print("\nğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­...")

    original_count = len(df)

    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã®ç¢ºèªã¨æ¨™æº–åŒ–
    expected_fields = {
        'ãƒ¬ã‚³ãƒ¼ãƒ‰ç•ªå·': 'record_id',
        'æ©Ÿç¨®': 'model',
        'å®¹é‡': 'capacity',
        'ãƒ©ãƒ³ã‚¯': 'rank',
        'æœ€çµ‚è²·å–ä¾¡æ ¼': 'price',
        'é€²æ—': 'status'
    }

    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
    missing_fields = [f for f in expected_fields.keys() if f not in df.columns]
    if missing_fields:
        print(f"âš ï¸  è­¦å‘Š: ä»¥ä¸‹ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing_fields}")
        print(f"   å®Ÿéš›ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰: {list(df.columns)}")
        sys.exit(1)

    # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰åã‚’è‹±èªã«å¤‰æ›
    df = df.rename(columns=expected_fields)

    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒç©ºã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å¤–
    df = df.dropna(subset=['model', 'capacity', 'rank', 'price'])
    print(f"  - ç©ºãƒ‡ãƒ¼ã‚¿é™¤å¤–: {original_count - len(df):,}ä»¶å‰Šé™¤")

    # iPhoneã®ã¿æŠ½å‡º
    original_count = len(df)
    df = df[df['model'].str.contains('iPhone', case=False, na=False)]
    print(f"  - iPhoneæŠ½å‡º: {original_count - len(df):,}ä»¶é™¤å¤–ï¼ˆéiPhoneï¼‰")

    # ä¾¡æ ¼ã‚’æ•°å€¤åŒ–
    df['price'] = pd.to_numeric(df['price'], errors='coerce')
    original_count = len(df)
    df = df.dropna(subset=['price'])
    print(f"  - ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼: {original_count - len(df):,}ä»¶é™¤å¤–ï¼ˆä¾¡æ ¼ç„¡åŠ¹ï¼‰")

    # ä¾¡æ ¼ãŒ0å††ã®ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
    original_count = len(df)
    df = df[df['price'] > 0]
    print(f"  - 0å††ãƒ‡ãƒ¼ã‚¿é™¤å¤–: {original_count - len(df):,}ä»¶å‰Šé™¤")

    print(f"âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {len(df):,}ä»¶")

    return df

def normalize_model_name(model: str) -> str:
    """
    æ©Ÿç¨®åã‚’æ­£è¦åŒ–

    ä¾‹:
        "iPhone 14 Pro" -> "iPhone 14 Pro"
        "iPhone SEï¼ˆç¬¬3ä¸–ä»£ï¼‰" -> "iPhone SE (3rd Gen)"
        "iPhone13" -> "iPhone 13"
    """
    import re

    # ã‚¹ãƒšãƒ¼ã‚¹ã®çµ±ä¸€
    model = model.strip()

    # ã€ŒiPhone13ã€â†’ã€ŒiPhone 13ã€ã®ã‚ˆã†ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
    model = re.sub(r'iPhone(\d+)', r'iPhone \1', model)

    # ä¸–ä»£è¡¨è¨˜ã®æ­£è¦åŒ–
    model = model.replace('ï¼ˆç¬¬1ä¸–ä»£ï¼‰', ' (1st Gen)')
    model = model.replace('ï¼ˆç¬¬2ä¸–ä»£ï¼‰', ' (2nd Gen)')
    model = model.replace('ï¼ˆç¬¬3ä¸–ä»£ï¼‰', ' (3rd Gen)')

    return model

def normalize_capacity(capacity: str) -> str:
    """
    å®¹é‡ã‚’æ­£è¦åŒ–

    ä¾‹:
        "128GB" -> "128GB"
        "128 GB" -> "128GB"
        "1TB" -> "1TB"
    """
    import re

    # ã‚¹ãƒšãƒ¼ã‚¹é™¤å»
    capacity = capacity.replace(' ', '')

    # å¤§æ–‡å­—ã«çµ±ä¸€
    capacity = capacity.upper()

    return capacity

def normalize_rank(rank: str) -> str:
    """
    ãƒ©ãƒ³ã‚¯ã‚’æ­£è¦åŒ–

    Kintoneã®ãƒ©ãƒ³ã‚¯ â†’ ç«¶åˆã‚µã‚¤ãƒˆã®ãƒ©ãƒ³ã‚¯è¡¨è¨˜ã«åˆã‚ã›ã‚‹:
    - æ–°å“ãƒ»æœªé–‹å° -> æ–°å“
    - æ–°å“åŒæ§˜ï¼ˆAï¼‰ -> A
    - ç¾å“ï¼ˆBï¼‰ -> B
    - ä½¿ç”¨æ„Ÿã‚ã‚Šï¼ˆCï¼‰ -> C
    - ç›®ç«‹ã¤å‚·ã‚ã‚Š -> Cï¼ˆç«¶åˆã‚µã‚¤ãƒˆã§ã¯Cãƒ©ãƒ³ã‚¯ç›¸å½“ï¼‰
    - å¤–è£…ã‚¸ãƒ£ãƒ³ã‚¯ -> ã‚¸ãƒ£ãƒ³ã‚¯
    """
    rank_mapping = {
        'æ–°å“ãƒ»æœªé–‹å°': 'æ–°å“',
        'æ–°å“åŒæ§˜ï¼ˆAï¼‰': 'A',
        'æ–°å“åŒæ§˜': 'A',
        'ç¾å“ï¼ˆBï¼‰': 'B',
        'ç¾å“': 'B',
        'ä½¿ç”¨æ„Ÿã‚ã‚Šï¼ˆCï¼‰': 'C',
        'ä½¿ç”¨æ„Ÿã‚ã‚Š': 'C',
        'ç›®ç«‹ã¤å‚·ã‚ã‚Š': 'C',
        'å‚·ã‚ã‚Š': 'C',
        'å¤–è£…ã‚¸ãƒ£ãƒ³ã‚¯': 'ã‚¸ãƒ£ãƒ³ã‚¯',
        'ã‚¸ãƒ£ãƒ³ã‚¯': 'ã‚¸ãƒ£ãƒ³ã‚¯',
    }

    return rank_mapping.get(rank, rank)

def extract_max_prices(df: pd.DataFrame) -> pd.DataFrame:
    """
    åŒä¸€ãƒ¢ãƒ‡ãƒ«ãƒ»å®¹é‡ãƒ»ãƒ©ãƒ³ã‚¯ã§æœ€é«˜ä¾¡æ ¼ã‚’æŠ½å‡º

    ç†ç”±: ã‚«ãƒ©ãƒ¼é•ã„ç­‰ã§åŒã˜ãƒ¢ãƒ‡ãƒ«ã§ã‚‚ä¾¡æ ¼ãŒç•°ãªã‚‹å ´åˆã€
          ç«¶åˆæ¯”è¼ƒã§ã¯æœ€é«˜ä¾¡æ ¼ã‚’æ¡ç”¨ã™ã‚‹
    """
    print("\nğŸ’° æœ€é«˜ä¾¡æ ¼æŠ½å‡ºä¸­...")

    original_count = len(df)

    # æ©Ÿç¨®åãƒ»å®¹é‡ãƒ»ãƒ©ãƒ³ã‚¯ã‚’æ­£è¦åŒ–
    df['model_normalized'] = df['model'].apply(normalize_model_name)
    df['capacity_normalized'] = df['capacity'].apply(normalize_capacity)
    df['rank_normalized'] = df['rank'].apply(normalize_rank)

    # åŒä¸€ãƒ¢ãƒ‡ãƒ«ãƒ»å®¹é‡ãƒ»ãƒ©ãƒ³ã‚¯ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦æœ€é«˜ä¾¡æ ¼ã‚’æŠ½å‡º
    df_max = df.loc[df.groupby(['model_normalized', 'capacity_normalized', 'rank_normalized'])['price'].idxmax()]

    print(f"  - å…ƒãƒ‡ãƒ¼ã‚¿: {original_count:,}ä»¶")
    print(f"  - æœ€é«˜ä¾¡æ ¼æŠ½å‡ºå¾Œ: {len(df_max):,}ä»¶")
    print(f"  - å‰Šæ¸›: {original_count - len(df_max):,}ä»¶")

    return df_max

def create_normalized_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    ç«¶åˆãƒ‡ãƒ¼ã‚¿å½¢å¼ã«æ­£è¦åŒ–

    å‡ºåŠ›å½¢å¼:
        - æ©Ÿç¨®å
        - å®¹é‡
        - è²·å–ãƒ©ãƒ³ã‚¯
        - ä¼æ¥­å: "å¼Šç¤¾"
        - è²·å–ä¾¡æ ¼
    """
    print("\nğŸ”„ ç«¶åˆãƒ‡ãƒ¼ã‚¿å½¢å¼ã«æ­£è¦åŒ–ä¸­...")

    normalized_df = pd.DataFrame({
        'æ©Ÿç¨®å': df['model_normalized'],
        'å®¹é‡': df['capacity_normalized'],
        'è²·å–ãƒ©ãƒ³ã‚¯': df['rank_normalized'],
        'ä¼æ¥­å': 'å¼Šç¤¾',
        'è²·å–ä¾¡æ ¼': df['price'].astype(int)
    })

    # ã‚½ãƒ¼ãƒˆ: æ©Ÿç¨®å -> å®¹é‡ -> ãƒ©ãƒ³ã‚¯
    normalized_df = normalized_df.sort_values(['æ©Ÿç¨®å', 'å®¹é‡', 'è²·å–ãƒ©ãƒ³ã‚¯'])

    # é‡è¤‡é™¤å¤–ï¼ˆå¿µã®ãŸã‚ï¼‰
    original_count = len(normalized_df)
    normalized_df = normalized_df.drop_duplicates()

    if original_count > len(normalized_df):
        print(f"  - é‡è¤‡é™¤å¤–: {original_count - len(normalized_df):,}ä»¶")

    print(f"âœ… æ­£è¦åŒ–å®Œäº†: {len(normalized_df):,}ä»¶")

    return normalized_df

def save_normalized_data(df: pd.DataFrame, output_dir: str = 'data/processed'):
    """
    æ­£è¦åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = output_path / 'normalized_internal_data.csv'
    output_file_timestamped = output_path / f'normalized_internal_data_{timestamp}.csv'

    # ä¿å­˜
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    df.to_csv(output_file_timestamped, index=False, encoding='utf-8-sig')

    print(f"\nğŸ’¾ ä¿å­˜å®Œäº†:")
    print(f"  - {output_file}")
    print(f"  - {output_file_timestamped}")

    return output_file

def show_statistics(df: pd.DataFrame):
    """
    ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    """
    print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ:")
    print(f"  - ç·ä»¶æ•°: {len(df):,}ä»¶")
    print(f"  - ãƒ¦ãƒ‹ãƒ¼ã‚¯æ©Ÿç¨®æ•°: {df['æ©Ÿç¨®å'].nunique()}æ©Ÿç¨®")
    print(f"  - ä¾¡æ ¼ç¯„å›²: Â¥{df['è²·å–ä¾¡æ ¼'].min():,} ã€œ Â¥{df['è²·å–ä¾¡æ ¼'].max():,}")
    print(f"  - å¹³å‡ä¾¡æ ¼: Â¥{df['è²·å–ä¾¡æ ¼'].mean():,.0f}")
    print(f"  - ä¸­å¤®å€¤: Â¥{df['è²·å–ä¾¡æ ¼'].median():,.0f}")

    print("\nğŸ“‹ ãƒ©ãƒ³ã‚¯åˆ¥ä»¶æ•°:")
    rank_counts = df['è²·å–ãƒ©ãƒ³ã‚¯'].value_counts()
    for rank, count in rank_counts.items():
        print(f"  - {rank}: {count:,}ä»¶")

    print("\nğŸ” è²·å–ä¾¡æ ¼TOP10:")
    top10 = df.nlargest(10, 'è²·å–ä¾¡æ ¼')[['æ©Ÿç¨®å', 'å®¹é‡', 'è²·å–ãƒ©ãƒ³ã‚¯', 'è²·å–ä¾¡æ ¼']]
    for idx, row in top10.iterrows():
        print(f"  {row['æ©Ÿç¨®å']} {row['å®¹é‡']} ({row['è²·å–ãƒ©ãƒ³ã‚¯']}): Â¥{row['è²·å–ä¾¡æ ¼']:,}")

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: uv run python scripts/process_internal_data.py <CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹>")
        print("ä¾‹: uv run python scripts/process_internal_data.py data/internal/kintone_buyback_data_20251117.csv")
        sys.exit(1)

    input_file = sys.argv[1]

    if not Path(input_file).exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_file}")
        sys.exit(1)

    print("=" * 60)
    print("å¼Šç¤¾è²·å–å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿å‡¦ç†")
    print("=" * 60)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    df = load_kintone_data(input_file)

    # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    df = clean_data(df)

    # æœ€é«˜ä¾¡æ ¼æŠ½å‡º
    df = extract_max_prices(df)

    # æ­£è¦åŒ–
    normalized_df = create_normalized_data(df)

    # çµ±è¨ˆæƒ…å ±è¡¨ç¤º
    show_statistics(normalized_df)

    # ä¿å­˜
    output_file = save_normalized_data(normalized_df)

    print("\n" + "=" * 60)
    print("âœ… å‡¦ç†å®Œäº†")
    print("=" * 60)
    print(f"\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"  1. ãƒ‡ãƒ¼ã‚¿ç¢ºèª:")
    print(f"     uv run python scripts/view_csv.py {output_file}")
    print(f"  2. 3ç¤¾æ¯”è¼ƒåˆ†æ:")
    print(f"     uv run python scripts/analyze_three_way.py")

if __name__ == '__main__':
    main()
