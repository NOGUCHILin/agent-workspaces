"""
ã‚²ã‚ªè²·å–ä¾¡æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆPlaywrightç‰ˆï¼‰

Akamai WAFã‚’å›é¿ã™ã‚‹ãŸã‚ã«é«˜åº¦ãªãƒ–ãƒ©ã‚¦ã‚¶è‡ªå‹•åŒ–æŠ€è¡“ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
"""

from browser_scraper_base import BrowserScraperBase
from playwright.sync_api import sync_playwright
from models import IPHONE_MODELS
from pathlib import Path
from datetime import datetime
import json
import time
import re


class GeoScraper:
    """
    ã‚²ã‚ªè²·å–ä¾¡æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼

    Playwrightã‚’ä½¿ç”¨ã—ã¦ã‚²ã‚ªã®WAFã‚’å›é¿ã—ãªãŒã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã™ã€‚
    """

    BASE_URL = "https://buymobile.geo-online.co.jp/"
    SEARCH_URL = "https://buymobile.geo-online.co.jp/mitsumori/"

    def __init__(self, headless: bool = False):
        """
        Args:
            headless: ãƒ˜ãƒƒãƒ‰ãƒ¬ã‚¹ãƒ¢ãƒ¼ãƒ‰ï¼ˆWAFå›é¿ã®ãŸã‚é€šå¸¸ã¯Falseæ¨å¥¨ï¼‰
        """
        self.headless = headless
        self.results = []

    def _setup_browser(self, p):
        """WAFå›é¿ã®ãŸã‚ã®ãƒ–ãƒ©ã‚¦ã‚¶è¨­å®š"""
        browser = p.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )

        context = browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent=(
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) '
                'AppleWebKit/537.36 (KHTML, like Gecko) '
                'Chrome/120.0.0.0 Safari/537.36'
            ),
            locale='ja-JP',
            timezone_id='Asia/Tokyo',
            extra_http_headers={
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept-Encoding': 'gzip, deflate, br',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            },
        )

        return browser, context

    def _add_stealth_scripts(self, page):
        """è‡ªå‹•åŒ–æ¤œå‡ºã‚’å›é¿ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’è¿½åŠ """
        page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            Object.defineProperty(navigator, 'languages', {
                get: () => ['ja-JP', 'ja', 'en-US', 'en'],
            });
            delete navigator.__proto__.webdriver;
        """)

    def scrape_model(self, model: str, capacity: str) -> list[dict]:
        """
        æŒ‡å®šãƒ¢ãƒ‡ãƒ«ãƒ»å®¹é‡ã®è²·å–ä¾¡æ ¼ã‚’å–å¾—

        Args:
            model: ãƒ¢ãƒ‡ãƒ«åï¼ˆä¾‹: "iPhone 15 Pro"ï¼‰
            capacity: å®¹é‡ï¼ˆä¾‹: "256GB"ï¼‰

        Returns:
            è²·å–ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        search_query = f"{model} {capacity}"
        print(f"\nğŸ” æ¤œç´¢: {search_query}")

        with sync_playwright() as p:
            browser, context = self._setup_browser(p)
            page = context.new_page()
            self._add_stealth_scripts(page)

            try:
                # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                print(f"ğŸ“„ {self.BASE_URL} ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")
                page.goto(self.BASE_URL, wait_until="domcontentloaded", timeout=60000)
                time.sleep(3)  # ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿ã‚’å¾…ã¤

                # ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‰ã˜ã‚‹ï¼ˆè¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                print("ğŸ”² ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‰ã˜ã¾ã™...")
                try:
                    # ãƒ¢ãƒ¼ãƒ€ãƒ«ã®é–‰ã˜ã‚‹ãƒœã‚¿ãƒ³ã‚’æ¢ã™
                    close_button = page.query_selector('.modal .close, .modal button, .modal [class*="close"]')
                    if close_button and close_button.is_visible():
                        close_button.click()
                        time.sleep(1)
                        print("  âœ“ ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‰ã˜ã¾ã—ãŸ")
                    else:
                        # ãƒ¢ãƒ¼ãƒ€ãƒ«èƒŒæ™¯ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼ˆã¾ãŸã¯ Escape ã‚­ãƒ¼ï¼‰
                        page.keyboard.press('Escape')
                        time.sleep(1)
                        print("  âœ“ Escapeã‚­ãƒ¼ã§ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’é–‰ã˜ã¾ã—ãŸ")
                except Exception as e:
                    print(f"  âš ï¸ ãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã‚¹ã‚­ãƒƒãƒ—: {e}")

                # æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã«å…¥åŠ›
                print(f"âŒ¨ï¸  æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã«å…¥åŠ›: {search_query}")
                page.fill('input[name="search1"]', search_query)
                time.sleep(1)

                # æ¤œç´¢å®Ÿè¡Œ
                print("ğŸ” æ¤œç´¢å®Ÿè¡Œ...")
                page.click('button[type="submit"]')
                page.wait_for_load_state("domcontentloaded", timeout=60000)
                time.sleep(4)  # æ¤œç´¢çµæœã®è¡¨ç¤ºã‚’å¾…ã¤

                # æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã®URL
                current_url = page.url
                print(f"ğŸ“ ç¾åœ¨ã®URL: {current_url}")

                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                screenshot_dir = Path(__file__).parent.parent / "data" / "screenshots"
                screenshot_dir.mkdir(parents=True, exist_ok=True)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                safe_filename = search_query.replace(" ", "_")
                screenshot_path = screenshot_dir / f"geo_search_{safe_filename}_{timestamp}.png"
                page.screenshot(path=str(screenshot_path))
                print(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ: {screenshot_path}")

                # è²·å–ä¾¡æ ¼ã‚’æŠ½å‡º
                results = self._extract_prices(page, model, capacity, current_url)

                print(f"âœ… {len(results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—")
                return results

            except Exception as e:
                print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                return []

            finally:
                browser.close()

    def _extract_prices(self, page, model: str, capacity: str, url: str) -> list[dict]:
        """ãƒšãƒ¼ã‚¸ã‹ã‚‰è²·å–ä¾¡æ ¼ã‚’æŠ½å‡º"""
        results = []
        timestamp = datetime.now().isoformat()

        try:
            # å•†å“ã‚«ãƒ¼ãƒ‰ã‚’æ¢ã™ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™ï¼‰
            selectors = [
                '.product-item',
                '.product-card',
                '.item',
                'article',
                '[class*="product"]',
            ]

            products = []
            for selector in selectors:
                products = page.query_selector_all(selector)
                if products:
                    print(f"  âœ“ ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ '{selector}' ã§{len(products)}ä»¶ç™ºè¦‹")
                    break

            if not products:
                print("  âš ï¸ å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                # HTMLã‚’ä¿å­˜ã—ã¦ãƒ‡ãƒãƒƒã‚°
                html_path = Path(__file__).parent.parent / "data" / "debug" / f"geo_no_results_{timestamp}.html"
                html_path.parent.mkdir(parents=True, exist_ok=True)
                html_path.write_text(page.content(), encoding='utf-8')
                print(f"  ğŸ’¾ ãƒ‡ãƒãƒƒã‚°ç”¨HTMLä¿å­˜: {html_path}")
                return results

            # å„å•†å“ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            for i, product in enumerate(products[:20], 1):  # æœ€åˆã®20ä»¶ã¾ã§
                try:
                    # å•†å“å
                    name_elem = product.query_selector('h2, h3, .title, [class*="name"], [class*="title"]')
                    product_name = name_elem.inner_text().strip() if name_elem else "ä¸æ˜"

                    # ä¾¡æ ¼ï¼ˆè¤‡æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™ï¼‰
                    price_elem = product.query_selector('.price, [class*="price"], strong')
                    price_text = price_elem.inner_text() if price_elem else ""

                    # ä¾¡æ ¼ã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
                    price_match = re.search(r'([0-9,]+)å††', price_text)
                    if price_match:
                        buyback_price = int(price_match.group(1).replace(',', ''))
                    else:
                        print(f"  âš ï¸ å•†å“{i}: ä¾¡æ ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆ{product_name}ï¼‰")
                        continue

                    # è©³ç´°ãƒšãƒ¼ã‚¸ã®URL
                    link_elem = product.query_selector('a')
                    detail_url = link_elem.get_attribute('href') if link_elem else ""
                    if detail_url and detail_url.startswith('/'):
                        detail_url = self.BASE_URL.rstrip('/') + detail_url

                    # çŠ¶æ…‹ã‚’æ¨æ¸¬ï¼ˆæœªä½¿ç”¨å“ã€ä¸­å¤ãªã©ï¼‰
                    condition = "ä¸­å¤å“"
                    if "æœªä½¿ç”¨" in product_name:
                        condition = "æœªä½¿ç”¨å“"
                    elif "æ–°å“" in product_name:
                        condition = "æ–°å“"

                    result = {
                        "product_name": product_name,
                        "model": model,
                        "capacity": capacity,
                        "condition": condition,
                        "buyback_price": buyback_price,
                        "url": detail_url or url,
                        "site": "ã‚²ã‚ª",
                        "scraped_at": timestamp,
                    }

                    results.append(result)
                    print(f"  {i}. {product_name}: Â¥{buyback_price:,}")

                except Exception as e:
                    print(f"  âš ï¸ å•†å“{i}ã®è§£æã‚¨ãƒ©ãƒ¼: {e}")
                    continue

        except Exception as e:
            print(f"  âŒ ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {e}")

        return results

    def scrape_test_models(self) -> list[dict]:
        """
        ãƒ†ã‚¹ãƒˆç”¨ï¼š3ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Returns:
            åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        """
        test_models = [
            ("iPhone X", "64GB"),
            ("iPhone X", "256GB"),
            ("iPhone XR", "64GB"),
        ]

        all_results = []

        print("=" * 60)
        print("ã‚²ã‚ªè²·å–ä¾¡æ ¼ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼‰")
        print("=" * 60)

        for model, capacity in test_models:
            results = self.scrape_model(model, capacity)
            all_results.extend(results)

            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å¾…æ©Ÿ
            time.sleep(5)

        # çµæœã‚’ä¿å­˜
        if all_results:
            output_dir = Path(__file__).parent.parent / "data" / "raw" / "geo"
            output_dir.mkdir(parents=True, exist_ok=True)

            for model, capacity in test_models:
                model_results = [r for r in all_results if r["model"] == model and r["capacity"] == capacity]
                if model_results:
                    filename = f"geo_{model.replace(' ', '_')}_{capacity}.json"
                    output_path = output_dir / filename
                    with open(output_path, 'w', encoding='utf-8') as f:
                        json.dump(model_results, f, ensure_ascii=False, indent=2)
                    print(f"\nğŸ’¾ ä¿å­˜: {output_path}")

        print(f"\nâœ… å®Œäº†: {len(all_results)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†")
        return all_results


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    scraper = GeoScraper(headless=False)  # WAFå›é¿ã®ãŸã‚é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
    results = scraper.scrape_test_models()

    if results:
        print("\nğŸ“Š åé›†ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼:")
        for r in results:
            print(f"  - {r['model']} {r['capacity']}: Â¥{r['buyback_price']:,}")


if __name__ == "__main__":
    main()
